{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file contains functions to be used in the LDA Reading Recommender code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Patrick Steeves for Independent Study with Professor Kanungo <br>\n",
    "George Washington University, 12/23/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data from GitHub as 6 separate zipped files. Combine files and compute wordcounts for all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def importData():\n",
    "    url = \"https://github.com/psteeves/NLP-projects/raw/master/LDA%20Reading%20Recommender/Data/\"\n",
    "\n",
    "    textnums = range(7)   # Number of files to import\n",
    "    temp = []\n",
    "    for i in textnums:\n",
    "        print(\"Importing and unzipping file {}/{}...\".format(i+1, textnums[-1]+1))\n",
    "        file = 'articles'+str(i)+'.zip'\n",
    "        urlretrieve(url+file, filename=file)\n",
    "\n",
    "        zip_ref = zipfile.ZipFile(file, 'r')\n",
    "        zip_ref.extractall()\n",
    "        zip_ref.close()\n",
    "\n",
    "        temp.append(pd.read_csv(file.replace('zip','csv'), encoding = 'utf-8'))\n",
    "\n",
    "    data = pd.concat(temp).reset_index(drop=True)\n",
    "\n",
    "    print(\"Computing word counts\")\n",
    "    data['word_count'] = data.content.apply(lambda x: len(x.split()))\n",
    "    data = data.drop(['Unnamed: 0','id','author','date','year','month','url'],axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Clean each article by tokenizing and lemmatizing words and filtering stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanData(data,text_column):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"Starting cleaning...\")\n",
    "    print(\"Tokenizing...\")\n",
    "    data['tokens'] = [tokenizer.tokenize(text.lower()) for text in data[text_column]]\n",
    "    print(\"Lemmatizing and filtering stopwords...\")\n",
    "    data['tokens'] = [[lemmatizer.lemmatize(token) for token in text if len(token) > 1 and token not in stopwords.words('english')] for text in data['tokens']]\n",
    "\n",
    "    print(\"Took {:.0f} minutes to clean texts\".format((time.time()-start)/60))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Recognize and add bigrams to articles, such as New York, North Korea, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addBigrams(df, token_column):\n",
    "    phrases = Phrases(df[token_column], min_count = 150)\n",
    "    bigrams = Phraser(phrases)\n",
    "    for idx in range(len(df[token_column])):\n",
    "        for token in bigrams[df[token_column][idx]]:\n",
    "            if '_' in token:\n",
    "                df[token_column][idx].append(token)\n",
    "    return bigrams, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Transform alphabetical tokenized articles into bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createBOW(df,token_column):\n",
    "    dictionary = Dictionary(df.loc[:,token_column])\n",
    "    dictionary.filter_extremes(no_below = 150, no_above = 0.6)\n",
    "    df['bow'] = [dictionary.doc2bow(doc) for doc in df[token_column]]\n",
    "    return df, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Given a series of articles as bag of words, compute PDF for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDocTopics(bow_series, model):\n",
    "    topic_pairs = bow_series.apply(lambda x: model.get_document_topics(x, minimum_probability = 1e-8))\n",
    "    topics = [np.array([prob[1] for prob in row]) for row in topic_pairs]\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>When given a series of numbers, return the second, third, and fourth smallest values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smallestNums(series):\n",
    "    position1 = None\n",
    "    position2 = None    # position of 2nd smallest num\n",
    "    position3 = None    # position of 3rd smallest num\n",
    "    position4 = None    # position of 4th smallest num\n",
    "    step = 0\n",
    "    m1, m2, m3, m4 = float('inf'), float('inf'), float('inf'), float('inf')\n",
    "    for num in series:\n",
    "        if num <= m1:\n",
    "            m1, m2, m3, m4 = num, m1, m2, m3\n",
    "            position4 = position3\n",
    "            position3 = position2\n",
    "            position2 = position1\n",
    "            position1 = step\n",
    "        elif num < m2:\n",
    "            m2, m3, m4 = num, m2, m3\n",
    "            position4 = position3\n",
    "            position3 = position2\n",
    "            position2 = step\n",
    "        elif num < m3:\n",
    "            m3, m4 = num, m3\n",
    "            position4 = position3\n",
    "            position3 = step\n",
    "        elif num < m4:\n",
    "            m4 = num\n",
    "            position4 = step\n",
    "        step += 1\n",
    "    return position2, position3, position4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>When fed the index of an article, compute the Euclidian distance between each article's PDF and the given article's PDF. Then return the second, third, and fourth smallest distances (the smallest distance is with itself and will equal 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarArticles(articlenum):\n",
    "    pdf = training_data.iloc[articlenum,6]\n",
    "    diff = training_data.topics.apply(lambda x: np.linalg.norm(pdf - x))\n",
    "    return smallestNums(diff)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
