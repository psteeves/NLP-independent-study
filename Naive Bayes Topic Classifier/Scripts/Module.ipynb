{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file contains functions used in the Naive Bayes Classifier code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Patrick Steeves for Independent Study with Professor Kanungo <br>\n",
    "George Washington University, 12/23/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def importData():\n",
    "    # Pull data from repository\n",
    "    url = \"https://github.com/psteeves/NLP-projects/raw/master/Naive%20Bayes%20Topic%20Classifier/Data/\"\n",
    "    urllib.request.urlretrieve(url+'news1.zip', filename='news1.zip')\n",
    "    urllib.request.urlretrieve(url+'news2.zip', filename='news2.zip')\n",
    "\n",
    "    # Unzip CSV files\n",
    "    zip_ref = zipfile.ZipFile('news1.zip', 'r')\n",
    "    zip_ref.extractall()\n",
    "    zip_ref.close()\n",
    "\n",
    "    zip_ref = zipfile.ZipFile('news2.zip', 'r')\n",
    "    zip_ref.extractall()\n",
    "    zip_ref.close()\n",
    "    \n",
    "    news1 = pd.read_csv('news1.csv', encoding = 'latin1')\n",
    "    news2 = pd.read_csv('news2.csv', encoding = 'latin1')\n",
    "\n",
    "    return pd.concat([news1, news2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Clean titles by stemming and removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanWords(df, title_col):\n",
    "\n",
    "    start = time.time()   # Time how long cleaning takes\n",
    "    stopped_words = []   # List of titles converted to stopped words\n",
    "\n",
    "    print(\"Started cleaning headlines...\")\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    checkpoint = time.time()\n",
    "    for idx, row in df[title_col].iteritems():\n",
    "        cleaned_title = re.sub('[^a-zA-Z]+',' ', row).lower()    # Only keep alphabetical characters\n",
    "        words = [stemmer.stem(word) for word in cleaned_title.split() if word not in stopwords.words('english')]   # Stem and filter stopwords\n",
    "        stopped_words.append(','.join(words))   # Append cleaned words to list of cleaned titles\n",
    "        \n",
    "        if time.time() - checkpoint > 600:   # Update user on progress every 10min\n",
    "            print(\"Done cleaning {:2.1f}% of headlines\".format(100*idx/len(df)))\n",
    "            checkpoint = time.time()\n",
    "\n",
    "    headlines['STOPPED_WORDS'] = stopped_words   # Add cleaned column to dataframe\n",
    "\n",
    "    print(\"Took {:4.1f} minutes to clean titles\".format((time.time()-start)/60))\n",
    "    \n",
    "    return headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Classifier trained on given series of titles and of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NBClassifier:\n",
    "    \"\"\"\n",
    "    Naive Bayes Classifiertrained to perform topic classification\n",
    "    \"\"\"\n",
    "    def __init__(self, titles, categories, train_split = 1):\n",
    "        self.data = pd.concat([titles,categories], axis=1)              # Dataframe of cleaned titles and cats\n",
    "        train_idx = np.random.rand(len(self.data)) < train_split        # Training data rows\n",
    "        self.train_data = self.data.loc[train_idx,:].copy()\n",
    "        self.test_data = self.data.loc[~train_idx,:].copy()\n",
    "        \n",
    "        all_words = []       # All words in titles, including duplicates\n",
    "        for row in titles:\n",
    "            all_words += row.split(',')\n",
    "\n",
    "        self.total_words = len(all_words)\n",
    "        self.word_count = dict(Counter(all_words))\n",
    "        self.common_words = {w:c for w,c in self.word_count.items() if c > 5}  # Only keep words that appear at least 10 times\n",
    "        self.unique_words = self.common_words.keys()\n",
    "        self.categories = set(categories)\n",
    "        self.pdf = {}               # Word counts over all categories, to be trained later\n",
    "        self.words_per_cat = {}     # Number of words per category\n",
    "        \n",
    "        self.train_accuracy = None\n",
    "        self.test_accuracy = None\n",
    "        self.trained = False        # Indicator if classifier has already been trained\n",
    "        self.misclassified = None   # Misclassified titles from test set, or from training test if no train/test split\n",
    "\n",
    "        \n",
    "    def trainPDF(self):\n",
    "        \"\"\"\n",
    "        Update word count and total number of words in each category\n",
    "        \"\"\"\n",
    "        i = 1\n",
    "        for cat in self.categories:\n",
    "            print(\"Creating PDf for topic {}/{}\".format(i,len(self.categories)))\n",
    "            relevant_cat = self.train_data.loc[lambda df: df.iloc[:,1] == cat,:]\n",
    "            self.words_per_cat[cat] = 0\n",
    "            self.pdf[cat]={}\n",
    "            for row in relevant_cat.iloc[:,0]:\n",
    "                title_words = row.split(',')\n",
    "                self.words_per_cat[cat] += len(title_words)     # Iteratively number of words\n",
    "                for word in title_words:\n",
    "                    if self.pdf[cat].get(word):\n",
    "                        self.pdf[cat][word] += 1       # For every word in title, iteratively update word count for category\n",
    "                    else:\n",
    "                        self.pdf[cat][word] = 1        # If word has not been seen already in category, set count to 1\n",
    "            i+=1\n",
    "            \n",
    "        self.trained = True\n",
    "\n",
    "\n",
    "    def getProb(cat, word, laplace_smooth):\n",
    "        \"\"\"\n",
    "        Compute probability of category conditioned on word, adding Laplacian smoothing\n",
    "        \"\"\"\n",
    "    prob = self.pdf[cat].get(word)\n",
    "    return ((0 if prob is None else prob) + laplace_smooth) / (self.words_per_cat[cat] + laplace_smooth*self.total_words)\n",
    "\n",
    "\n",
    "    def predictCats(self, title, already_stopped = False):\n",
    "        \"\"\"\n",
    "        Given a title, predict topic of article. Assumes that title is not already cleaned.\n",
    "        Returns dictionary of category probabilities for title.\n",
    "        \"\"\"\n",
    "        if already_stopped:   # If title fed is already stopped\n",
    "            words = [word for word in title.split(',')]\n",
    "        else:\n",
    "            stemmer = PorterStemmer()\n",
    "            cleaned_title = re.sub('[^a-zA-Z]+',' ', title).lower()\n",
    "            words = [stemmer.stem(word) for word in cleaned_title.split() if word not in stopwords.words('english')]\n",
    "        preds = {}\n",
    "        for cat in self.categories:\n",
    "            preds[cat] = 0\n",
    "            for word in words:\n",
    "                # Get probability for each category using Naive Bayes\n",
    "                preds[cat] += math.log(self.getProb(cat, word, 1))\n",
    "\n",
    "        return preds\n",
    "\n",
    "    \n",
    "    def classifyData(self):\n",
    "        \"\"\"\n",
    "        Using trained Classifier, predict training and testing data.\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            print(\"You must train the classifier first\")\n",
    "            return\n",
    "        \n",
    "        print(\"Predicting train data\")\n",
    "        self.train_data.loc[:,'PREDICTED'] = ''\n",
    "        start_min = time.time()\n",
    "        \n",
    "        for idx, row in self.train_data.iterrows():\n",
    "            tr_predictions = self.predictCats(row.iloc[0], already_stopped = True)\n",
    "            self.train_data.loc[idx,'PREDICTED'] = max(tr_predictions, key = tr_predictions.get)\n",
    "\n",
    "            if time.time() - start_min > 120:     # Update user on progress every 2 minutes\n",
    "                print(\"{:2.1f}% complete\".format(100*idx/self.train_data.index[-1]))\n",
    "                start_min = time.time()\n",
    "\n",
    "        if len(self.test_data) > 0:\n",
    "            self.test_data.loc[:,'PREDICTED'] = ''\n",
    "            print(\"Predicting test data\")\n",
    "            start_min = time.time()\n",
    "            for idx, row in self.test_data.iterrows():\n",
    "                te_predictions = self.predictCats(row.iloc[0], already_stopped = True)\n",
    "                self.test_data.loc[idx,'PREDICTED'] = max(te_predictions, key = te_predictions.get)\n",
    "\n",
    "                if time.time() - start_min > 120:\n",
    "                    print(\"{:2.1f}% complete\".format(100*idx/self.train_data.index[-1]))\n",
    "                    start_min = time.time()\n",
    "\n",
    "        self.train_accuracy = sum(self.train_data.PREDICTED == self.train_data.CATEGORY) / len(self.train_data)\n",
    "        if len(self.test_data > 0):\n",
    "            self.test_accuracy = sum(self.test_data.PREDICTED == self.test_data.CATEGORY) / len(self.test_data)\n",
    "            self.misclassified = self.test_data.loc[self.test_data['PREDICTED'] != self.test_data.iloc[:,1]]\n",
    "        else:\n",
    "            self.misclassified = self.train_data.loc[self.train_data['PREDICTED'] != self.train_data.iloc[:,1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
