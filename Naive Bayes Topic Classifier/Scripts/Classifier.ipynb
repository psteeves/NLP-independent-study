{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Patrick Steeves as part of Independent Study with Professor Kanungo<br>\n",
    "George Washington University 12/23/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project trains a NB classifier on news article headlines to classify articles by topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be found at https://www.kaggle.com/uciml/news-aggregator-dataset <br>\n",
    "The data contains 400,000 headlines from news stories in 2014 in one of 4 categories: health, business, science and tech, entertainment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import nececssary modules and define URL for downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "import re\n",
    "\n",
    "url = \"https://github.com/psteeves/NLP-projects/raw/master/Naive%20Bayes%20Topic%20Classifier/Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was almost 100MB in size, so it was split into two files and zipped before being uploaded to Github. The lines below download and unzip the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename1, headers1 = urllib.request.urlretrieve(url+'news1.zip', filename='news1.zip')\n",
    "filename2, headers2 = urllib.request.urlretrieve(url+'news2.zip', filename='news2.zip')\n",
    "\n",
    "zip_ref = zipfile.ZipFile('news1.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "zip_ref = zipfile.ZipFile('news2.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Read in the data and concatenate both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news1 = pd.read_csv('news1.csv', encoding = 'latin1')\n",
    "news2 = pd.read_csv('news2.csv', encoding = 'latin1')\n",
    "\n",
    "frames = [news1, news2]\n",
    "titles = pd.concat(frames).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data below. As we can see in the CATEGORY column, titles have the following categories: <br>\n",
    "b - business <br>\n",
    "e - entertainment <br>\n",
    "t - science and technology <br>\n",
    "m - health <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STORY</th>\n",
       "      <th>HOSTNAME</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.latimes.com</td>\n",
       "      <td>1394470370698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.livemint.com</td>\n",
       "      <td>1394470371207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/us-open-stocks...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/fed-risks-fall...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>http://www.moneynews.com/Economy/federal-reser...</td>\n",
       "      <td>Moneynews</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.moneynews.com</td>\n",
       "      <td>1394470372027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1  Fed official says weak data caused by weather,...   \n",
       "1   2  Fed's Charles Plosser sees high bar for change...   \n",
       "2   3  US open: Stocks fall after Fed official hints ...   \n",
       "3   4  Fed risks falling 'behind the curve', Charles ...   \n",
       "4   5  Fed's Plosser: Nasty Weather Has Curbed Job Gr...   \n",
       "\n",
       "                                                 URL          PUBLISHER  \\\n",
       "0  http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n",
       "1  http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n",
       "2  http://www.ifamagazine.com/news/us-open-stocks...       IFA Magazine   \n",
       "3  http://www.ifamagazine.com/news/fed-risks-fall...       IFA Magazine   \n",
       "4  http://www.moneynews.com/Economy/federal-reser...          Moneynews   \n",
       "\n",
       "  CATEGORY                          STORY             HOSTNAME      TIMESTAMP  \n",
       "0        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM      www.latimes.com  1394470370698  \n",
       "1        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM     www.livemint.com  1394470371207  \n",
       "2        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371550  \n",
       "3        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371793  \n",
       "4        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM    www.moneynews.com  1394470372027  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below cleans titles by dropping unnecessary columns, removing punctuation and stopwords, and stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanWords(df):\n",
    "    headlines = df.drop(['URL','STORY','TIMESTAMP','HOSTNAME','ID'], axis=1)\n",
    "\n",
    "    start = time.time()   # Time how long cleaning takes\n",
    "    stopped_words = []   # List of titles converted to stopped words\n",
    "\n",
    "    print(\"Started cleaning headlines...\")\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    checkpoint = time.time()\n",
    "    for idx, row in df['TITLE'].iteritems():\n",
    "        cleaned_title = re.sub('[^a-zA-Z]+',' ', row).lower()    # Only keep alphabetical characters\n",
    "        words = [stemmer.stem(word) for word in cleaned_title.split() if word not in stopwords.words('english')]   # Stem and filter stopwords\n",
    "        stopped_words.append(','.join(words))   # Append cleaned words to list of cleaned titles\n",
    "        \n",
    "        if time.time() - checkpoint > 600:   # Update user on progress every 10min\n",
    "            print(\"Done cleaning {:2.1f}% of headlines\".format(100*idx/len(df)))\n",
    "            checkpoint = time.time()\n",
    "\n",
    "    headlines['STOPPED_WORDS'] = stopped_words   # Add cleaned column to dataframe\n",
    "\n",
    "    headlines.to_csv(\"cleaned_titles.csv\",index=False)\n",
    "\n",
    "    print(\"Took {:4.1f} minutes to clean titles\".format((time.time()-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the use the option of either cleaning data using above function (and return a CSV of cleaned titles) or download the pre-cleaned titles from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download = 0   # Set to 1 to download pre-cleaned titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started cleaning headlines...\n",
      "Done cleaning 6.1% of headlines\n",
      "Done cleaning 14.2% of headlines\n",
      "Done cleaning 22.4% of headlines\n",
      "Done cleaning 30.9% of headlines\n",
      "Done cleaning 38.8% of headlines\n",
      "Done cleaning 46.7% of headlines\n",
      "Done cleaning 55.0% of headlines\n",
      "Done cleaning 63.3% of headlines\n",
      "Done cleaning 71.5% of headlines\n",
      "Done cleaning 79.7% of headlines\n",
      "Done cleaning 87.9% of headlines\n",
      "Done cleaning 96.1% of headlines\n",
      "Took 125.1 minutes to clean titles\n"
     ]
    }
   ],
   "source": [
    "if download:\n",
    "    filename3, headers3 = urllib.request.urlretrieve(url+'cleaned_titles.zip', filename='cleaned_titles.zip')\n",
    "    zip_ref = zipfile.ZipFile('cleaned_titles.zip', 'r')\n",
    "    zip_ref.extractall()\n",
    "    zip_ref.close()\n",
    "    \n",
    "else:\n",
    "    cleanWords(titles)\n",
    "\n",
    "headlines = pd.read_csv('cleaned_titles.csv', encoding = 'latin1', keep_default_na = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STOPPED_WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>fed,offici,say,weak,data,caus,weather,slow,taper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>fed,charl,plosser,see,high,bar,chang,pace,taper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>us,open,stock,fall,fed,offici,hint,acceler,taper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE          PUBLISHER  \\\n",
       "0  Fed official says weak data caused by weather,...  Los Angeles Times   \n",
       "1  Fed's Charles Plosser sees high bar for change...           Livemint   \n",
       "2  US open: Stocks fall after Fed official hints ...       IFA Magazine   \n",
       "\n",
       "  CATEGORY                                     STOPPED_WORDS  \n",
       "0        b  fed,offici,say,weak,data,caus,weather,slow,taper  \n",
       "1        b   fed,charl,plosser,see,high,bar,chang,pace,taper  \n",
       "2        b  us,open,stock,fall,fed,offici,hint,acceler,taper  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.iloc[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import additional packages needed for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given word counts over topics, calculate P(category | word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getProb(pdf, words_in_cat, cat, word, total_words):\n",
    "    # words_in_cat is number of unique words in category, total_words is total number of words in all categories\n",
    "    # pdf is the word counts over the categories\n",
    "    laplace_smooth = 2\n",
    "    prob = pdf.get(word)\n",
    "    return ((0 if prob is None else prob) + laplace_smooth) / (words_in_cat + laplace_smooth*total_words)    # If word is not in PDF of cat, first term in expression is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Build class for NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NBClassifier:\n",
    "    \"\"\"\n",
    "    Naive Bayes Classifier. Given Pandas Series of article titles and categories, learns to perform topic classification\n",
    "    \"\"\"\n",
    "    def __init__(self, titles, categories, train_split = 1):\n",
    "        self.data = pd.concat([titles,categories], axis=1)              # Dataframe of cleaned titles and cats\n",
    "        train_idx = np.random.rand(len(self.data)) < train_split        # Training data rows\n",
    "        self.train_data = self.data.loc[train_idx,:].copy()\n",
    "        self.test_data = self.data.loc[~train_idx,:].copy()\n",
    "        \n",
    "        all_words = []       # All words in titles, including duplicates\n",
    "        for row in titles:\n",
    "            all_words += row.split(',')\n",
    "\n",
    "        self.total_words = len(all_words)\n",
    "        self.word_count = dict(Counter(all_words))\n",
    "        self.common_words = {w:c for w,c in self.word_count.items() if c > 5}  # Only keep words that appear at least 10 times\n",
    "        self.unique_words = self.common_words.keys()\n",
    "        self.categories = set(categories)\n",
    "        self.pdf = {}               # Word counts over all categories, to be trained later\n",
    "        self.words_per_cat = {}     # Number of words per category\n",
    "        \n",
    "        self.train_accuracy = None\n",
    "        self.test_accuracy = None\n",
    "        self.trained = False        # Indicator if classifier has already been trained\n",
    "        self.misclassified = None   # Misclassified titles from test set, or from training test if no train/test split\n",
    "\n",
    "        \n",
    "    def trainPDF(self):\n",
    "        \"\"\"\n",
    "        Update word count and total number of words in each category\n",
    "        \"\"\"\n",
    "        i = 1\n",
    "        for cat in self.categories:\n",
    "            print(\"Creating PDf for category {}/{}\".format(i,len(self.categories)))\n",
    "            relevant_cat = self.train_data.loc[lambda df: df.iloc[:,1] == cat,:]\n",
    "            self.words_per_cat[cat] = 0\n",
    "            self.pdf[cat]={}\n",
    "            for row in relevant_cat.iloc[:,0]:\n",
    "                title_words = row.split(',')\n",
    "                self.words_per_cat[cat] += len(title_words)     # Iteratively number of words\n",
    "                for word in title_words:\n",
    "                    if self.pdf[cat].get(word):\n",
    "                        self.pdf[cat][word] += 1       # For every word in title, iteratively update word count for category\n",
    "                    else:\n",
    "                        self.pdf[cat][word] = 1        # If word has not been seen already in category, set count to 1\n",
    "            i+=1\n",
    "            \n",
    "        self.trained = True\n",
    "\n",
    "        \n",
    "    def predictCats(self, title, already_stopped = False):\n",
    "        \"\"\"\n",
    "        Given a title, predict category of article. Assumes that title is not already cleaned. Returns dictionary of category probabilities for title.\n",
    "        \"\"\"\n",
    "        if already_stopped:   # If title fed is already stopped\n",
    "            words = [word for word in title.split(',')]\n",
    "        else:\n",
    "            stemmer = PorterStemmer()\n",
    "            cleaned_title = re.sub('[^a-zA-Z]+',' ', title).lower()\n",
    "            words = [stemmer.stem(word) for word in cleaned_title.split() if word not in stopwords.words('english')]\n",
    "        preds = {}\n",
    "        for cat in self.categories:\n",
    "            preds[cat] = 0\n",
    "            for word in words:\n",
    "                # Get probability for each category using Naive Bayes\n",
    "                preds[cat] += math.log(getProb(self.pdf[cat], self.words_per_cat[cat], cat, word, self.total_words))\n",
    "\n",
    "        return preds\n",
    "\n",
    "    \n",
    "    def classifyData(self):\n",
    "        \"\"\"\n",
    "        Using trained Classifier, predict training and testing data.\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            print(\"You must train the classifier first\")\n",
    "            return\n",
    "        \n",
    "        print(\"Predicting train data\")\n",
    "        self.train_data.loc[:,'PREDICTED'] = ''\n",
    "        start_min = time.time()\n",
    "        \n",
    "        for idx, row in self.train_data.iterrows():\n",
    "            tr_predictions = self.predictCats(row.iloc[0], already_stopped = True)\n",
    "            self.train_data.loc[idx,'PREDICTED'] = max(tr_predictions, key = tr_predictions.get)  # Predicted cat is one with highest probability\n",
    "            \n",
    "            if time.time() - start_min > 120:     # Update user on progress every 2 minutes\n",
    "                print(\"{:2.1f}% complete\".format(100*idx/self.train_data.index[-1]))\n",
    "                start_min = time.time()\n",
    "\n",
    "        if len(self.test_data) > 0:\n",
    "            self.test_data.loc[:,'PREDICTED'] = ''\n",
    "            print(\"Predicting test data\")\n",
    "            start_min = time.time()\n",
    "            for idx, row in self.test_data.iterrows():\n",
    "                te_predictions = self.predictCats(row.iloc[0], already_stopped = True)\n",
    "                self.test_data.loc[idx,'PREDICTED'] = max(te_predictions, key = te_predictions.get)\n",
    "\n",
    "                if time.time() - start_min > 120:\n",
    "                    print(\"{:2.1f}% complete\".format(100*idx/self.train_data.index[-1]))\n",
    "                    start_min = time.time()\n",
    "\n",
    "        self.train_accuracy = sum(self.train_data.PREDICTED == self.train_data.CATEGORY) / len(self.train_data)\n",
    "        if len(self.test_data > 0):\n",
    "            self.test_accuracy = sum(self.test_data.PREDICTED == self.test_data.CATEGORY) / len(self.test_data)\n",
    "            self.misclassified = self.test_data.loc[self.test_data['PREDICTED'] != self.test_data.iloc[:,1]]\n",
    "        else:\n",
    "            self.misclassified = self.train_data.loc[self.train_data['PREDICTED'] != self.train_data.iloc[:,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NBClassifier(headlines['STOPPED_WORDS'], headlines['CATEGORY'], train_split = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PDf for category 1/4\n",
      "Creating PDf for category 2/4\n",
      "Creating PDf for category 3/4\n",
      "Creating PDf for category 4/4\n"
     ]
    }
   ],
   "source": [
    "classifier.trainPDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting train data\n",
      "19.0% complete\n",
      "39.1% complete\n",
      "57.6% complete\n",
      "77.4% complete\n",
      "96.9% complete\n",
      "Predicting test data\n",
      "78.3% complete\n"
     ]
    }
   ],
   "source": [
    "classifier.classifyData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909904575237\n",
      "0.904140977488\n"
     ]
    }
   ],
   "source": [
    "print(classifier.train_accuracy)\n",
    "print(classifier.test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show examples of misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STOPPED_WORDS</th>\n",
       "      <th>PREDICTED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10 Things You Need To Know Before The Opening ...</td>\n",
       "      <td>b</td>\n",
       "      <td>thing,need,know,open,bell</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Why eBay Spinning Off Paypal Makes Sense</td>\n",
       "      <td>b</td>\n",
       "      <td>ebay,spin,paypal,make,sens</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Happy 5th birthday, bull market</td>\n",
       "      <td>b</td>\n",
       "      <td>happi,th,birthday,bull,market</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Hackers Leak Mt. Gox Database, Reveal Blog of ...</td>\n",
       "      <td>b</td>\n",
       "      <td>hacker,leak,mt,gox,databas,reveal,blog,former,ceo</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Massive hacking attacks revealed</td>\n",
       "      <td>b</td>\n",
       "      <td>massiv,hack,attack,reveal</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Get on the bus, Gus: US posts record public tr...</td>\n",
       "      <td>b</td>\n",
       "      <td>get,bu,gu,us,post,record,public,transit,use</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>Rand Paul: I wouldn't let Putin get away with ...</td>\n",
       "      <td>b</td>\n",
       "      <td>rand,paul,let,putin,get,away</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>Window on Westminster</td>\n",
       "      <td>b</td>\n",
       "      <td>window,westminst</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>Paul Ryan calls for sanctions against Russian ...</td>\n",
       "      <td>b</td>\n",
       "      <td>paul,ryan,call,sanction,russian,oligarch</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>Orion Death Stars Destroy Planets Before They ...</td>\n",
       "      <td>t</td>\n",
       "      <td>orion,death,star,destroy,planet,even,form</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>Real-life ?Death Stars? deadlier than Darth Va...</td>\n",
       "      <td>t</td>\n",
       "      <td>real,life,death,star,deadlier,darth,vader,astr...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>'Death Stars' Wreck Havoc in the Orion Nebula</td>\n",
       "      <td>t</td>\n",
       "      <td>death,star,wreck,havoc,orion,nebula</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>Elephants recognise human voices</td>\n",
       "      <td>t</td>\n",
       "      <td>eleph,recognis,human,voic</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>Elephants really are intelligent: Creatures ca...</td>\n",
       "      <td>t</td>\n",
       "      <td>eleph,realli,intellig,creatur,guess,age,even,e...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>Mt. Gox files bankruptcy in US and faces more ...</td>\n",
       "      <td>t</td>\n",
       "      <td>mt,gox,file,bankruptci,us,face,hack</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>Mt. Gox Rumored To Have Pocketed Bitcoins Repo...</td>\n",
       "      <td>t</td>\n",
       "      <td>mt,gox,rumor,pocket,bitcoin,report,stolen</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>Bitcoin Exchange Mt. Gox Moves to Protect US A...</td>\n",
       "      <td>t</td>\n",
       "      <td>bitcoin,exchang,mt,gox,move,protect,us,asset</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>Mt. Gox files for US bankruptcy</td>\n",
       "      <td>t</td>\n",
       "      <td>mt,gox,file,us,bankruptci</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>Mt. Gox files US bankruptcy case</td>\n",
       "      <td>t</td>\n",
       "      <td>mt,gox,file,us,bankruptci,case</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>Bitcoin exchange files for US bankruptcy</td>\n",
       "      <td>t</td>\n",
       "      <td>bitcoin,exchang,file,us,bankruptci</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TITLE CATEGORY  \\\n",
       "34    10 Things You Need To Know Before The Opening ...        b   \n",
       "49             Why eBay Spinning Off Paypal Makes Sense        b   \n",
       "178                     Happy 5th birthday, bull market        b   \n",
       "225   Hackers Leak Mt. Gox Database, Reveal Blog of ...        b   \n",
       "280                    Massive hacking attacks revealed        b   \n",
       "428   Get on the bus, Gus: US posts record public tr...        b   \n",
       "641   Rand Paul: I wouldn't let Putin get away with ...        b   \n",
       "655                               Window on Westminster        b   \n",
       "672   Paul Ryan calls for sanctions against Russian ...        b   \n",
       "1109  Orion Death Stars Destroy Planets Before They ...        t   \n",
       "1110  Real-life ?Death Stars? deadlier than Darth Va...        t   \n",
       "1120      'Death Stars' Wreck Havoc in the Orion Nebula        t   \n",
       "1138                   Elephants recognise human voices        t   \n",
       "1154  Elephants really are intelligent: Creatures ca...        t   \n",
       "1164  Mt. Gox files bankruptcy in US and faces more ...        t   \n",
       "1167  Mt. Gox Rumored To Have Pocketed Bitcoins Repo...        t   \n",
       "1170  Bitcoin Exchange Mt. Gox Moves to Protect US A...        t   \n",
       "1173                    Mt. Gox files for US bankruptcy        t   \n",
       "1174                   Mt. Gox files US bankruptcy case        t   \n",
       "1175           Bitcoin exchange files for US bankruptcy        t   \n",
       "\n",
       "                                          STOPPED_WORDS PREDICTED  \n",
       "34                            thing,need,know,open,bell         e  \n",
       "49                           ebay,spin,paypal,make,sens         t  \n",
       "178                       happi,th,birthday,bull,market         e  \n",
       "225   hacker,leak,mt,gox,databas,reveal,blog,former,ceo         t  \n",
       "280                           massiv,hack,attack,reveal         t  \n",
       "428         get,bu,gu,us,post,record,public,transit,use         e  \n",
       "641                        rand,paul,let,putin,get,away         e  \n",
       "655                                    window,westminst         t  \n",
       "672            paul,ryan,call,sanction,russian,oligarch         e  \n",
       "1109          orion,death,star,destroy,planet,even,form         e  \n",
       "1110  real,life,death,star,deadlier,darth,vader,astr...         e  \n",
       "1120                death,star,wreck,havoc,orion,nebula         e  \n",
       "1138                          eleph,recognis,human,voic         e  \n",
       "1154  eleph,realli,intellig,creatur,guess,age,even,e...         e  \n",
       "1164                mt,gox,file,bankruptci,us,face,hack         b  \n",
       "1167          mt,gox,rumor,pocket,bitcoin,report,stolen         b  \n",
       "1170       bitcoin,exchang,mt,gox,move,protect,us,asset         b  \n",
       "1173                          mt,gox,file,us,bankruptci         b  \n",
       "1174                     mt,gox,file,us,bankruptci,case         b  \n",
       "1175                 bitcoin,exchang,file,us,bankruptci         b  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = list(classifier.misclassified.index)[:20]\n",
    "pd.concat([headlines.iloc[indices,[0,2,3]], classifier.misclassified.iloc[:20,2]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if classifier also works on random title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': -44.812632743367146, 't': -48.022133511231836, 'm': -58.28771720505842, 'e': -55.31794230064467}\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predictCats('Apple posts higher returns this quarter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, business has the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': -71.59521386580798, 't': -67.5631847361503, 'm': -74.84158060779703, 'e': -55.5692844385924}\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predictCats('Tom Cruise stars in upcoming action movie'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entertainment wins!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
