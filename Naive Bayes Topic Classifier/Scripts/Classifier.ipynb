{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Patrick Steeves as part of Independent Study with Professor Kanungo<br>\n",
    "George Washington University 12/23/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project trains a NB classifier on news article headlines to classify articles by topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be found at https://www.kaggle.com/uciml/news-aggregator-dataset <br>\n",
    "The data contains 400,000 headlines from news stories in 2014 in one of 4 categories: health, business, science and tech, entertainment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import nececssary modules and define URL for downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "import re\n",
    "\n",
    "url = \"https://github.com/psteeves/NLP-projects/raw/master/Naive%20Bayes%20Topic%20Classifier/Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was almost 100MB in size, so it was split into two files and zipped before being uploaded to Hithub. The lines below download and unzip the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename1, headers1 = urllib.request.urlretrieve(url+'news1.zip', filename='news1.zip')\n",
    "filename2, headers2 = urllib.request.urlretrieve(url+'news2.zip', filename='news2.zip')\n",
    "\n",
    "#Unzip first file\n",
    "zip_ref = zipfile.ZipFile('news1.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "#Unzip second file\n",
    "zip_ref = zipfile.ZipFile('news2.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Read in the data and concatenate both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news1 = pd.read_csv('news1.csv', encoding = 'latin1')\n",
    "news2 = pd.read_csv('news2.csv', encoding = 'latin1')\n",
    "\n",
    "frames = [news1, news2]\n",
    "titles = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data below. As we can see in the CATEGORY column, titles have the following categories: <br>\n",
    "b - business <br>\n",
    "e - entertainment <br>\n",
    "t - science and technology <br>\n",
    "m - health <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STORY</th>\n",
       "      <th>HOSTNAME</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.latimes.com</td>\n",
       "      <td>1394470370698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.livemint.com</td>\n",
       "      <td>1394470371207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/us-open-stocks...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/fed-risks-fall...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>http://www.moneynews.com/Economy/federal-reser...</td>\n",
       "      <td>Moneynews</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.moneynews.com</td>\n",
       "      <td>1394470372027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1  Fed official says weak data caused by weather,...   \n",
       "1   2  Fed's Charles Plosser sees high bar for change...   \n",
       "2   3  US open: Stocks fall after Fed official hints ...   \n",
       "3   4  Fed risks falling 'behind the curve', Charles ...   \n",
       "4   5  Fed's Plosser: Nasty Weather Has Curbed Job Gr...   \n",
       "\n",
       "                                                 URL          PUBLISHER  \\\n",
       "0  http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n",
       "1  http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n",
       "2  http://www.ifamagazine.com/news/us-open-stocks...       IFA Magazine   \n",
       "3  http://www.ifamagazine.com/news/fed-risks-fall...       IFA Magazine   \n",
       "4  http://www.moneynews.com/Economy/federal-reser...          Moneynews   \n",
       "\n",
       "  CATEGORY                          STORY             HOSTNAME      TIMESTAMP  \n",
       "0        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM      www.latimes.com  1394470370698  \n",
       "1        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM     www.livemint.com  1394470371207  \n",
       "2        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371550  \n",
       "3        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371793  \n",
       "4        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM    www.moneynews.com  1394470372027  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below cleans titles by dropping unnecessary columns, removing punctuation and stopwords, and stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanWords(df):\n",
    "    headlines = df.drop(['URL','STORY','TIMESTAMP','HOSTNAME','ID'], axis=1)\n",
    "\n",
    "    start = time.time()   # Time how long cleaning takes\n",
    "    stopped_words = []   # List of titles converted to stopped words\n",
    "\n",
    "    print(\"Started cleaning headlines...\")\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    checkpoint = time.time()\n",
    "    for idx, row in df['TITLE'].iteritems():\n",
    "        cleaned_title = re.sub('[^a-zA-Z]+',' ', row).lower()    # Only keep alphabetical characters\n",
    "        words = [stemmer.stem(word) for word in cleaned_title.split() if word not in stopwords.words('english')]   # Stem and filter stopwords\n",
    "        stopped_words.append(','.join(words))   # Append cleaned words to list of cleaned titles\n",
    "        \n",
    "        if time.time() - checkpoint > 600:   # Update user on progress every 10min\n",
    "            print(\"Done cleaning {:2.1f}% of headlines\".format(100*idx/len(df)))\n",
    "            checkpoint = time.time()\n",
    "\n",
    "    headlines['STOPPED_WORDS'] = stopped_words   # Add cleaned column to dataframe\n",
    "\n",
    "    headlines.to_csv(\"cleaned_titles.csv\",index=False)\n",
    "\n",
    "    print(\"Took {:4.1f} minutes to clean titles\".format((time.time()-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the use the option of either cleaning data using above function (and return a CSV of cleaned titles) or download the pre-cleaned titles from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download = 0   # Set to 1 to download pre-cleaned titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started cleaning headlines...\n",
      "Done cleaning 30000 headlines\n",
      "Done cleaning 60000 headlines\n",
      "Done cleaning 90000 headlines\n",
      "Done cleaning 120000 headlines\n",
      "Done cleaning 150000 headlines\n",
      "Done cleaning 180000 headlines\n",
      "Done cleaning 210000 headlines\n",
      "Done cleaning 240000 headlines\n",
      "Done cleaning 270000 headlines\n",
      "Done cleaning 300000 headlines\n",
      "Done cleaning 330000 headlines\n",
      "Done cleaning 360000 headlines\n",
      "Done cleaning 390000 headlines\n",
      "Done cleaning 420000 headlines\n",
      "Took 9731.95 seconds to create CSV file of cleaned titles\n"
     ]
    }
   ],
   "source": [
    "if download:\n",
    "    filename3, headers3 = urllib.request.urlretrieve(url+'cleaned_titles.csv', filename='cleaned_titles.csv')\n",
    "else:\n",
    "    cleanWords(titles)\n",
    "\n",
    "headlines = pd.read_csv('cleaned_titles.csv', encoding = 'latin1', keep_default_na = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STOPPED_WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>fed,offici,say,weak,data,caus,weather,slow,taper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>fed,charl,plosser,see,high,bar,chang,pace,taper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>us,open,stock,fall,fed,offici,hint,acceler,taper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE          PUBLISHER  \\\n",
       "0  Fed official says weak data caused by weather,...  Los Angeles Times   \n",
       "1  Fed's Charles Plosser sees high bar for change...           Livemint   \n",
       "2  US open: Stocks fall after Fed official hints ...       IFA Magazine   \n",
       "\n",
       "  CATEGORY                                     STOPPED_WORDS  \n",
       "0        b  fed,offici,say,weak,data,caus,weather,slow,taper  \n",
       "1        b   fed,charl,plosser,see,high,bar,chang,pace,taper  \n",
       "2        b  us,open,stock,fall,fed,offici,hint,acceler,taper  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.iloc[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import additional packages needed for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given word counts over topics, calculate P(category | word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getProb(pdf, words_in_cat, cat, word, total_words):\n",
    "    # words_in_cat is number of unique words in category, total_words is total number of words in all categories\n",
    "    # pdf is the word counts over the categories\n",
    "    laplace_smooth = 3\n",
    "    prob = pdf.get(word)\n",
    "    return ((0 if prob is None else prob) + laplace_smooth) / (words_in_cat + laplace_smooth*total_words)    # If word is not in PDF of cat, first term in expression is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Build class for NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NBClassifier:\n",
    "    \"\"\"\n",
    "    Naive Bayes Classifier. Given Pandas Series of article titles and categories, learns to perform topic classification\n",
    "    \"\"\"\n",
    "    def __init__(self, titles, categories, train_split = 1):\n",
    "        self.data = pd.concat([titles,categories], axis=1)              # Dataframe of cleaned titles and cats\n",
    "        train_idx = np.random.rand(len(self.data)) < train_split        # Training data rows\n",
    "        self.train_data = self.data.loc[train_idx,:].copy()\n",
    "        self.test_data = self.data.loc[~train_idx,:].copy()\n",
    "        \n",
    "        all_words = []       # All words in titles, including duplicates\n",
    "        for row in titles:\n",
    "            all_words += row.split(',')\n",
    "\n",
    "        self.total_words = len(all_words)\n",
    "        self.word_count = dict(Counter(all_words))\n",
    "        self.common_words = {w:c for w,c in self.word_count.items() if c > 10}  # Only keep words that appear at least 10 times\n",
    "        self.unique_words = self.common_words.keys()\n",
    "        self.categories = set(categories)\n",
    "        self.pdf = {}               # Word counts over all categories, to be trained later\n",
    "        self.words_per_cat = {}     # Number of words per category\n",
    "        \n",
    "        self.train_accuracy = None\n",
    "        self.test_accuracy = None\n",
    "        self.trained = False        # Indicator if classifier has already been trained\n",
    "        self.misclassified = None   # Misclassified titles from test set, or from training test if no train/test split\n",
    "\n",
    "        \n",
    "    def trainPDF(self):\n",
    "        \"\"\"\n",
    "        Update word count and total number of words in each category\n",
    "        \"\"\"\n",
    "        i = 1\n",
    "        for cat in self.categories:\n",
    "            print(\"Creating PDf for category {}/{}\".format(i,len(self.categories)))\n",
    "            relevant_cat = self.train_data.loc[lambda df: df.iloc[:,1] == cat,:]\n",
    "            self.words_per_cat[cat] = 0\n",
    "            self.pdf[cat]={}\n",
    "            for row in relevant_cat.iloc[:,0]:\n",
    "                title_words = row.split(',')\n",
    "                self.words_per_cat[cat] += len(title_words)     # Iteratively number of words\n",
    "                for word in title_words:\n",
    "                    if self.pdf[cat].get(word):\n",
    "                        self.pdf[cat][word] += 1       # For every word in title, iteratively update word count for category\n",
    "                    else:\n",
    "                        self.pdf[cat][word] = 1        # If word has not been seen already in category, set count to 1\n",
    "            i+=1\n",
    "            \n",
    "        self.trained = True\n",
    "\n",
    "        \n",
    "    def predictCats(self, title, already_stopped = False):\n",
    "        \"\"\"\n",
    "        Given a title, predict category of article. Assumes that title is not already cleaned. Returns dictionary of category probabilities for title.\n",
    "        \"\"\"\n",
    "        if already_stopped:   # If title fed is already stopped\n",
    "            words = [word for word in title.split(',')]\n",
    "        else:\n",
    "            stemmer = PorterStemmer()\n",
    "            cleaned_title = re.sub('[^a-zA-Z]+',' ', title).lower()\n",
    "            words = [stemmer.stem(word) for word in cleaned_title.split() if word not in stopwords.words('english')]\n",
    "        preds = {}\n",
    "        for cat in self.categories:\n",
    "            preds[cat] = 0\n",
    "            for word in words:\n",
    "                # Get probability for each category using Naive Bayes\n",
    "                preds[cat] += math.log(getProb(self.pdf[cat], self.words_per_cat[cat], cat, word, self.total_words))\n",
    "\n",
    "        return preds\n",
    "\n",
    "    \n",
    "    def classifyData(self):\n",
    "        \"\"\"\n",
    "        Using trained Classifier, predict training and testing data.\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            print(\"You must train the classifier first\")\n",
    "            return\n",
    "        \n",
    "        print(\"Predicting train data\")\n",
    "        self.train_data.loc[:,'PREDICTED'] = ''\n",
    "        start_min = time.time()\n",
    "        \n",
    "        for idx, row in self.train_data.iterrows():\n",
    "            tr_predictions = self.predictCats(row.iloc[0], already_stopped = True)\n",
    "            self.train_data.loc[idx,'PREDICTED'] = max(tr_predictions, key = tr_predictions.get)  # Predicted cat is one with highest probability\n",
    "            \n",
    "            if time.time() - start_min > 120:     # Update user on progress every 2 minutes\n",
    "                print(\"{:2.1f}% complete\".format(100*idx/self.train_data.index[-1]))\n",
    "                start_min = time.time()\n",
    "\n",
    "        if len(self.test_data) > 0:\n",
    "            self.test_data.loc[:,'PREDICTED'] = ''\n",
    "            print(\"Predicting test data\")\n",
    "            start_min = time.time()\n",
    "            for idx, row in self.test_data.iterrows():\n",
    "                te_predictions = self.predictCats(row.iloc[0], already_stopped = True)\n",
    "                self.test_data.loc[idx,'PREDICTED'] = max(te_predictions, key = te_predictions.get)\n",
    "\n",
    "                if time.time() - start_min > 120:\n",
    "                    print(\"{:2.1f}% complete\".format(100*idx/self.train_data.index[-1]))\n",
    "                    start_min = time.time()\n",
    "\n",
    "        self.train_accuracy = sum(self.train_data.PREDICTED == self.train_data.CATEGORY) / len(self.train_data)\n",
    "        if len(self.test_data > 0):\n",
    "            self.test_accuracy = sum(self.test_data.PREDICTED == self.test_data.CATEGORY) / len(self.test_data)\n",
    "            self.misclassified = self.test_data.loc[self.test_data['PREDICTED'] != self.test_data.iloc[:,1]]\n",
    "        else:\n",
    "            self.misclassified = self.train_data.loc[self.train_data['PREDICTED'] != self.train_data.iloc[:,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NBClassifier(headlines['STOPPED_WORDS'][:2000], headlines['CATEGORY'][:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PDf for category 1/2\n",
      "Creating PDf for category 2/2\n"
     ]
    }
   ],
   "source": [
    "classifier.trainPDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting train data\n"
     ]
    }
   ],
   "source": [
    "classifier.classifyData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.train_accuracy)\n",
    "print(classifier.test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show examples of misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STOPPED_WORDS</th>\n",
       "      <th>PREDICTED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10 Things You Need To Know This Morning</td>\n",
       "      <td>b</td>\n",
       "      <td>thing,need,know,morn</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Both ways</td>\n",
       "      <td>b</td>\n",
       "      <td>way</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3 Predictions for the New Week</td>\n",
       "      <td>b</td>\n",
       "      <td>predict,new,week</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>The Japanese government has come down hard on ...</td>\n",
       "      <td>b</td>\n",
       "      <td>japanes,govern,come,hard,digit,currenc,bitcoin</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Press Release: Pershing Square Issues Statement</td>\n",
       "      <td>b</td>\n",
       "      <td>press,releas,persh,squar,issu,statement</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>Finance News Update, what you need to know</td>\n",
       "      <td>b</td>\n",
       "      <td>financ,news,updat,need,know</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>Prodan: Price for Russian gas could stand at $...</td>\n",
       "      <td>b</td>\n",
       "      <td>prodan,price,russian,ga,could,stand</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>Eastern European countries paid much more for ...</td>\n",
       "      <td>b</td>\n",
       "      <td>eastern,european,countri,paid,much,russian,ga,...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>What's News?</td>\n",
       "      <td>t</td>\n",
       "      <td>news</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>Mt. Gox files bankruptcy in US and faces more ...</td>\n",
       "      <td>t</td>\n",
       "      <td>mt,gox,file,bankruptci,us,face,hack</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>Mt Gox files US bankruptcy, opponents call it ...</td>\n",
       "      <td>t</td>\n",
       "      <td>mt,gox,file,us,bankruptci,oppon,call,ruse</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>Following the Japan Filing, Now Mt. Gox Files ...</td>\n",
       "      <td>t</td>\n",
       "      <td>follow,japan,file,mt,gox,file,bankruptci,prote...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>Mt. Gox Rumored To Have Pocketed Bitcoins Repo...</td>\n",
       "      <td>t</td>\n",
       "      <td>mt,gox,rumor,pocket,bitcoin,report,stolen</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>Mt. Gox files for U.S. bankruptcy protection</td>\n",
       "      <td>t</td>\n",
       "      <td>mt,gox,file,u,bankruptci,protect</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>Bitcoin Exchange Mt. Gox Moves to Protect US A...</td>\n",
       "      <td>t</td>\n",
       "      <td>bitcoin,exchang,mt,gox,move,protect,us,asset</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>Fallout from collapse of Japanese bitcoin exch...</td>\n",
       "      <td>t</td>\n",
       "      <td>fallout,collaps,japanes,bitcoin,exchang,prompt...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>Bitcoin exchange Mt. Gox files US bankruptcy a...</td>\n",
       "      <td>t</td>\n",
       "      <td>bitcoin,exchang,mt,gox,file,us,bankruptci,amid...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>Mt. Gox files for US bankruptcy</td>\n",
       "      <td>t</td>\n",
       "      <td>mt,gox,file,us,bankruptci</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>Mt. Gox files US bankruptcy case</td>\n",
       "      <td>t</td>\n",
       "      <td>mt,gox,file,us,bankruptci,case</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>Bitcoin exchange files for US bankruptcy</td>\n",
       "      <td>t</td>\n",
       "      <td>bitcoin,exchang,file,us,bankruptci</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TITLE CATEGORY  \\\n",
       "24              10 Things You Need To Know This Morning        b   \n",
       "48                                            Both ways        b   \n",
       "167                      3 Predictions for the New Week        b   \n",
       "322   The Japanese government has come down hard on ...        b   \n",
       "355     Press Release: Pershing Square Issues Statement        b   \n",
       "574          Finance News Update, what you need to know        b   \n",
       "646   Prodan: Price for Russian gas could stand at $...        b   \n",
       "660   Eastern European countries paid much more for ...        b   \n",
       "1163                                       What's News?        t   \n",
       "1164  Mt. Gox files bankruptcy in US and faces more ...        t   \n",
       "1165  Mt Gox files US bankruptcy, opponents call it ...        t   \n",
       "1166  Following the Japan Filing, Now Mt. Gox Files ...        t   \n",
       "1167  Mt. Gox Rumored To Have Pocketed Bitcoins Repo...        t   \n",
       "1169       Mt. Gox files for U.S. bankruptcy protection        t   \n",
       "1170  Bitcoin Exchange Mt. Gox Moves to Protect US A...        t   \n",
       "1171  Fallout from collapse of Japanese bitcoin exch...        t   \n",
       "1172  Bitcoin exchange Mt. Gox files US bankruptcy a...        t   \n",
       "1173                    Mt. Gox files for US bankruptcy        t   \n",
       "1174                   Mt. Gox files US bankruptcy case        t   \n",
       "1175           Bitcoin exchange files for US bankruptcy        t   \n",
       "\n",
       "                                          STOPPED_WORDS PREDICTED  \n",
       "24                                 thing,need,know,morn         t  \n",
       "48                                                  way         t  \n",
       "167                                    predict,new,week         t  \n",
       "322      japanes,govern,come,hard,digit,currenc,bitcoin         t  \n",
       "355             press,releas,persh,squar,issu,statement         t  \n",
       "574                         financ,news,updat,need,know         t  \n",
       "646                 prodan,price,russian,ga,could,stand         t  \n",
       "660   eastern,european,countri,paid,much,russian,ga,...         t  \n",
       "1163                                               news         b  \n",
       "1164                mt,gox,file,bankruptci,us,face,hack         b  \n",
       "1165          mt,gox,file,us,bankruptci,oppon,call,ruse         b  \n",
       "1166  follow,japan,file,mt,gox,file,bankruptci,prote...         b  \n",
       "1167          mt,gox,rumor,pocket,bitcoin,report,stolen         b  \n",
       "1169                   mt,gox,file,u,bankruptci,protect         b  \n",
       "1170       bitcoin,exchang,mt,gox,move,protect,us,asset         b  \n",
       "1171  fallout,collaps,japanes,bitcoin,exchang,prompt...         b  \n",
       "1172  bitcoin,exchang,mt,gox,file,us,bankruptci,amid...         b  \n",
       "1173                          mt,gox,file,us,bankruptci         b  \n",
       "1174                     mt,gox,file,us,bankruptci,case         b  \n",
       "1175                 bitcoin,exchang,file,us,bankruptci         b  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = list(classifier.misclassified.index)[:20]\n",
    "pd.concat([headlines.iloc[indices,[0,2,3]], classifier.misclassified.iloc[:20,2]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if classifier also works on random title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classifier.predictCat('Apple posts higher returns this quarter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, business has the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classifier.predictCat('Tom Cruise stars in upcoming action movie'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entertainment wins!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
