{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Patrick Steeves as part of Independent Study with Professor Kanungo<br>\n",
    "George Washington University 12/23/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project trains a NB classifier on news article headlines to classify articles by topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be found at https://www.kaggle.com/uciml/news-aggregator-dataset <br>\n",
    "The data contains 400,000 headlines from news stories in 2014 in one of 4 categories: health, business, science and tech, entertainment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import nececssary modules and define URL for downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "import re\n",
    "\n",
    "url = \"https://github.com/psteeves/NLP-projects/raw/master/Naive%20Bayes%20Topic%20Classifier/Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was almost 100MB in size, so it was split into two files and zipped before being uploaded to Github. The lines below download and unzip the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename1, headers1 = urllib.request.urlretrieve(url+'news1.zip', filename='news1.zip')\n",
    "filename2, headers2 = urllib.request.urlretrieve(url+'news2.zip', filename='news2.zip')\n",
    "\n",
    "zip_ref = zipfile.ZipFile('news1.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "zip_ref = zipfile.ZipFile('news2.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Read in the data and concatenate both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news1 = pd.read_csv('news1.csv', encoding = 'latin1')\n",
    "news2 = pd.read_csv('news2.csv', encoding = 'latin1')\n",
    "\n",
    "frames = [news1, news2]\n",
    "titles = pd.concat(frames).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data below. As we can see in the CATEGORY column, titles have the following categories: <br>\n",
    "b - business <br>\n",
    "e - entertainment <br>\n",
    "t - science and technology <br>\n",
    "m - health <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STORY</th>\n",
       "      <th>HOSTNAME</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.latimes.com</td>\n",
       "      <td>1394470370698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.livemint.com</td>\n",
       "      <td>1394470371207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/us-open-stocks...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/fed-risks-fall...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>http://www.moneynews.com/Economy/federal-reser...</td>\n",
       "      <td>Moneynews</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.moneynews.com</td>\n",
       "      <td>1394470372027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1  Fed official says weak data caused by weather,...   \n",
       "1   2  Fed's Charles Plosser sees high bar for change...   \n",
       "2   3  US open: Stocks fall after Fed official hints ...   \n",
       "3   4  Fed risks falling 'behind the curve', Charles ...   \n",
       "4   5  Fed's Plosser: Nasty Weather Has Curbed Job Gr...   \n",
       "\n",
       "                                                 URL          PUBLISHER  \\\n",
       "0  http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n",
       "1  http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n",
       "2  http://www.ifamagazine.com/news/us-open-stocks...       IFA Magazine   \n",
       "3  http://www.ifamagazine.com/news/fed-risks-fall...       IFA Magazine   \n",
       "4  http://www.moneynews.com/Economy/federal-reser...          Moneynews   \n",
       "\n",
       "  CATEGORY                          STORY             HOSTNAME      TIMESTAMP  \n",
       "0        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM      www.latimes.com  1394470370698  \n",
       "1        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM     www.livemint.com  1394470371207  \n",
       "2        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371550  \n",
       "3        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371793  \n",
       "4        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM    www.moneynews.com  1394470372027  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below cleans titles by dropping unnecessary columns, removing punctuation and stopwords, and stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanWords(df):\n",
    "    headlines = df.drop(['URL','STORY','TIMESTAMP','HOSTNAME','ID'], axis=1)\n",
    "\n",
    "    start = time.time()   # Time how long cleaning takes\n",
    "    stopped_words = []   # List of titles converted to stopped words\n",
    "\n",
    "    print(\"Started cleaning headlines...\")\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    checkpoint = time.time()\n",
    "    for idx, row in df['TITLE'].iteritems():\n",
    "        cleaned_title = re.sub('[^a-zA-Z]+',' ', row).lower()    # Only keep alphabetical characters\n",
    "        words = [stemmer.stem(word) for word in cleaned_title.split() if word not in stopwords.words('english')]   # Stem and filter stopwords\n",
    "        stopped_words.append(','.join(words))   # Append cleaned words to list of cleaned titles\n",
    "        \n",
    "        if time.time() - checkpoint > 600:   # Update user on progress every 10min\n",
    "            print(\"Done cleaning {:2.1f}% of headlines\".format(100*idx/len(df)))\n",
    "            checkpoint = time.time()\n",
    "\n",
    "    headlines['STOPPED_WORDS'] = stopped_words   # Add cleaned column to dataframe\n",
    "\n",
    "    headlines.to_csv(\"cleaned_titles.csv\",index=False)\n",
    "\n",
    "    print(\"Took {:4.1f} minutes to clean titles\".format((time.time()-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the use the option of either cleaning data using above function (and return a CSV of cleaned titles) or download the pre-cleaned titles from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download = 0   # Set to 1 to download pre-cleaned titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started cleaning headlines...\n",
      "Done cleaning 6.0% of headlines\n",
      "Done cleaning 13.5% of headlines\n",
      "Done cleaning 21.1% of headlines\n",
      "Done cleaning 29.0% of headlines\n",
      "Done cleaning 37.4% of headlines\n",
      "Done cleaning 45.6% of headlines\n",
      "Done cleaning 53.8% of headlines\n",
      "Done cleaning 61.9% of headlines\n",
      "Done cleaning 70.1% of headlines\n",
      "Done cleaning 78.3% of headlines\n",
      "Done cleaning 86.6% of headlines\n",
      "Done cleaning 94.7% of headlines\n",
      "Took 127.4 minutes to clean titles\n"
     ]
    }
   ],
   "source": [
    "if download:\n",
    "    filename3, headers3 = urllib.request.urlretrieve(url+'cleaned_titles.zip', filename='cleaned_titles.zip')\n",
    "    zip_ref = zipfile.ZipFile('cleaned_titles.zip', 'r')\n",
    "    zip_ref.extractall()\n",
    "    zip_ref.close()\n",
    "    \n",
    "else:\n",
    "    cleanWords(titles)\n",
    "\n",
    "headlines = pd.read_csv('cleaned_titles.csv', encoding = 'latin1', keep_default_na = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STOPPED_WORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>fed,offici,say,weak,data,caus,weather,slow,taper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>fed,charl,plosser,see,high,bar,chang,pace,taper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>us,open,stock,fall,fed,offici,hint,acceler,taper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE          PUBLISHER  \\\n",
       "0  Fed official says weak data caused by weather,...  Los Angeles Times   \n",
       "1  Fed's Charles Plosser sees high bar for change...           Livemint   \n",
       "2  US open: Stocks fall after Fed official hints ...       IFA Magazine   \n",
       "\n",
       "  CATEGORY                                     STOPPED_WORDS  \n",
       "0        b  fed,offici,say,weak,data,caus,weather,slow,taper  \n",
       "1        b   fed,charl,plosser,see,high,bar,chang,pace,taper  \n",
       "2        b  us,open,stock,fall,fed,offici,hint,acceler,taper  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.iloc[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import additional packages needed for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given word counts over topics, calculate P(category | word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getProb(pdf, words_in_cat, cat, word, total_words):\n",
    "    # words_in_cat is number of unique words in category, total_words is total number of words in all categories\n",
    "    # pdf is the word counts over the categories\n",
    "    laplace_smooth = 2\n",
    "    prob = pdf.get(word)\n",
    "    return ((0 if prob is None else prob) + laplace_smooth) / (words_in_cat + laplace_smooth*total_words)    # If word is not in PDF of cat, first term in expression is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Build class for NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NBClassifier:\n",
    "    \"\"\"\n",
    "    Naive Bayes Classifier. Given Pandas Series of article titles and categories, learns to perform topic classification\n",
    "    \"\"\"\n",
    "    def __init__(self, titles, categories, train_split = 1):\n",
    "        self.data = pd.concat([titles,categories], axis=1)              # Dataframe of cleaned titles and cats\n",
    "        train_idx = np.random.rand(len(self.data)) < train_split        # Training data rows\n",
    "        self.train_data = self.data.loc[train_idx,:].copy()\n",
    "        self.test_data = self.data.loc[~train_idx,:].copy()\n",
    "        \n",
    "        all_words = []       # All words in titles, including duplicates\n",
    "        for row in titles:\n",
    "            all_words += row.split(',')\n",
    "\n",
    "        self.total_words = len(all_words)\n",
    "        self.word_count = dict(Counter(all_words))\n",
    "        self.common_words = {w:c for w,c in self.word_count.items() if c > 5}  # Only keep words that appear at least 10 times\n",
    "        self.unique_words = self.common_words.keys()\n",
    "        self.categories = set(categories)\n",
    "        self.pdf = {}               # Word counts over all categories, to be trained later\n",
    "        self.words_per_cat = {}     # Number of words per category\n",
    "        \n",
    "        self.train_accuracy = None\n",
    "        self.test_accuracy = None\n",
    "        self.trained = False        # Indicator if classifier has already been trained\n",
    "        self.misclassified = None   # Misclassified titles from test set, or from training test if no train/test split\n",
    "\n",
    "        \n",
    "    def trainPDF(self):\n",
    "        \"\"\"\n",
    "        Update word count and total number of words in each category\n",
    "        \"\"\"\n",
    "        i = 1\n",
    "        for cat in self.categories:\n",
    "            print(\"Creating PDf for category {}/{}\".format(i,len(self.categories)))\n",
    "            relevant_cat = self.train_data.loc[lambda df: df.iloc[:,1] == cat,:]\n",
    "            self.words_per_cat[cat] = 0\n",
    "            self.pdf[cat]={}\n",
    "            for row in relevant_cat.iloc[:,0]:\n",
    "                title_words = row.split(',')\n",
    "                self.words_per_cat[cat] += len(title_words)     # Iteratively number of words\n",
    "                for word in title_words:\n",
    "                    if self.pdf[cat].get(word):\n",
    "                        self.pdf[cat][word] += 1       # For every word in title, iteratively update word count for category\n",
    "                    else:\n",
    "                        self.pdf[cat][word] = 1        # If word has not been seen already in category, set count to 1\n",
    "            i+=1\n",
    "            \n",
    "        self.trained = True\n",
    "\n",
    "        \n",
    "    def predictCats(self, title, already_stopped = False):\n",
    "        \"\"\"\n",
    "        Given a title, predict category of article. Assumes that title is not already cleaned. Returns dictionary of category probabilities for title.\n",
    "        \"\"\"\n",
    "        if already_stopped:   # If title fed is already stopped\n",
    "            words = [word for word in title.split(',')]\n",
    "        else:\n",
    "            stemmer = PorterStemmer()\n",
    "            cleaned_title = re.sub('[^a-zA-Z]+',' ', title).lower()\n",
    "            words = [stemmer.stem(word) for word in cleaned_title.split() if word not in stopwords.words('english')]\n",
    "        preds = {}\n",
    "        for cat in self.categories:\n",
    "            preds[cat] = 0\n",
    "            for word in words:\n",
    "                # Get probability for each category using Naive Bayes\n",
    "                preds[cat] += math.log(getProb(self.pdf[cat], self.words_per_cat[cat], cat, word, self.total_words))\n",
    "\n",
    "        return preds\n",
    "\n",
    "    \n",
    "    def classifyData(self):\n",
    "        \"\"\"\n",
    "        Using trained Classifier, predict training and testing data.\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            print(\"You must train the classifier first\")\n",
    "            return\n",
    "        \n",
    "        print(\"Predicting train data\")\n",
    "        self.train_data.loc[:,'PREDICTED'] = ''\n",
    "        start_min = time.time()\n",
    "        \n",
    "        for idx, row in self.train_data.iterrows():\n",
    "            tr_predictions = self.predictCats(row.iloc[0], already_stopped = True)\n",
    "            self.train_data.loc[idx,'PREDICTED'] = max(tr_predictions, key = tr_predictions.get)  # Predicted cat is one with highest probability\n",
    "            \n",
    "            if time.time() - start_min > 120:     # Update user on progress every 2 minutes\n",
    "                print(\"{:2.1f}% complete\".format(100*idx/self.train_data.index[-1]))\n",
    "                start_min = time.time()\n",
    "\n",
    "        if len(self.test_data) > 0:\n",
    "            self.test_data.loc[:,'PREDICTED'] = ''\n",
    "            print(\"Predicting test data\")\n",
    "            start_min = time.time()\n",
    "            for idx, row in self.test_data.iterrows():\n",
    "                te_predictions = self.predictCats(row.iloc[0], already_stopped = True)\n",
    "                self.test_data.loc[idx,'PREDICTED'] = max(te_predictions, key = te_predictions.get)\n",
    "\n",
    "                if time.time() - start_min > 120:\n",
    "                    print(\"{:2.1f}% complete\".format(100*idx/self.train_data.index[-1]))\n",
    "                    start_min = time.time()\n",
    "\n",
    "        self.train_accuracy = sum(self.train_data.PREDICTED == self.train_data.CATEGORY) / len(self.train_data)\n",
    "        if len(self.test_data > 0):\n",
    "            self.test_accuracy = sum(self.test_data.PREDICTED == self.test_data.CATEGORY) / len(self.test_data)\n",
    "            self.misclassified = self.test_data.loc[self.test_data['PREDICTED'] != self.test_data.iloc[:,1]]\n",
    "        else:\n",
    "            self.misclassified = self.train_data.loc[self.train_data['PREDICTED'] != self.train_data.iloc[:,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NBClassifier(headlines['STOPPED_WORDS'], headlines['CATEGORY'], train_split = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PDf for category 1/4\n",
      "Creating PDf for category 2/4\n",
      "Creating PDf for category 3/4\n",
      "Creating PDf for category 4/4\n"
     ]
    }
   ],
   "source": [
    "classifier.trainPDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting train data\n",
      "7.0% complete\n",
      "14.3% complete\n",
      "26.7% complete\n",
      "39.6% complete\n",
      "54.5% complete\n",
      "71.4% complete\n",
      "87.0% complete\n",
      "Predicting test data\n",
      "57.2% complete\n"
     ]
    }
   ],
   "source": [
    "classifier.classifyData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903681371462\n",
      "0.90069446096\n"
     ]
    }
   ],
   "source": [
    "print(classifier.train_accuracy)\n",
    "print(classifier.test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show examples of misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STOPPED_WORDS</th>\n",
       "      <th>PREDICTED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10 Things You Need To Know This Morning</td>\n",
       "      <td>b</td>\n",
       "      <td>thing,need,know,morn</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10 Things You Need To Know Before The Opening ...</td>\n",
       "      <td>b</td>\n",
       "      <td>thing,need,know,open,bell</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Both ways</td>\n",
       "      <td>b</td>\n",
       "      <td>way</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Why eBay Spinning Off Paypal Makes Sense</td>\n",
       "      <td>b</td>\n",
       "      <td>ebay,spin,paypal,make,sens</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Bull market celebrates its 5th birthday: But i...</td>\n",
       "      <td>b</td>\n",
       "      <td>bull,market,celebr,th,birthday,parti</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3 Predictions for the New Week</td>\n",
       "      <td>b</td>\n",
       "      <td>predict,new,week</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Happy 5th birthday, bull market</td>\n",
       "      <td>b</td>\n",
       "      <td>happi,th,birthday,bull,market</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Bull market marks 5th birthday</td>\n",
       "      <td>b</td>\n",
       "      <td>bull,market,mark,th,birthday</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Happy Birthday, Bull</td>\n",
       "      <td>b</td>\n",
       "      <td>happi,birthday,bull</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Hackers Leak Mt. Gox Database, Reveal Blog of ...</td>\n",
       "      <td>b</td>\n",
       "      <td>hacker,leak,mt,gox,databas,reveal,blog,former,ceo</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>Hackers hit web accounts of MtGox boss</td>\n",
       "      <td>b</td>\n",
       "      <td>hacker,hit,web,account,mtgox,boss</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Mt. Gox CEO's blog hacked, database leak claim...</td>\n",
       "      <td>b</td>\n",
       "      <td>mt,gox,ceo,blog,hack,databas,leak,claim,k,bitcoin</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Massive hacking attacks revealed</td>\n",
       "      <td>b</td>\n",
       "      <td>massiv,hack,attack,reveal</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Anonymous hackers uncover alleged proof of MtG...</td>\n",
       "      <td>b</td>\n",
       "      <td>anonym,hacker,uncov,alleg,proof,mtgox,fraud,si...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>Metro-North Worked Killed in Harlem</td>\n",
       "      <td>b</td>\n",
       "      <td>metro,north,work,kill,harlem</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>Without a Trace</td>\n",
       "      <td>b</td>\n",
       "      <td>without,trace</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>American Airlines &amp; JetBlue: Le Divorce</td>\n",
       "      <td>b</td>\n",
       "      <td>american,airlin,jetblu,le,divorc</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>JetBlue airplanes at their gates at John F. Ke...</td>\n",
       "      <td>b</td>\n",
       "      <td>jetblu,airplan,gate,john,f,kennedi,airport,new...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>JetBlue and American Airlines go their differe...</td>\n",
       "      <td>b</td>\n",
       "      <td>jetblu,american,airlin,go,differ,way</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>Is spitting in TriMet driver's face 'assault'?...</td>\n",
       "      <td>b</td>\n",
       "      <td>spit,trimet,driver,face,assault,union,want,hos...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 TITLE CATEGORY  \\\n",
       "24             10 Things You Need To Know This Morning        b   \n",
       "34   10 Things You Need To Know Before The Opening ...        b   \n",
       "48                                           Both ways        b   \n",
       "49            Why eBay Spinning Off Paypal Makes Sense        b   \n",
       "133  Bull market celebrates its 5th birthday: But i...        b   \n",
       "167                     3 Predictions for the New Week        b   \n",
       "178                    Happy 5th birthday, bull market        b   \n",
       "187                     Bull market marks 5th birthday        b   \n",
       "192                               Happy Birthday, Bull        b   \n",
       "225  Hackers Leak Mt. Gox Database, Reveal Blog of ...        b   \n",
       "254             Hackers hit web accounts of MtGox boss        b   \n",
       "263  Mt. Gox CEO's blog hacked, database leak claim...        b   \n",
       "280                   Massive hacking attacks revealed        b   \n",
       "283  Anonymous hackers uncover alleged proof of MtG...        b   \n",
       "335                Metro-North Worked Killed in Harlem        b   \n",
       "369                                    Without a Trace        b   \n",
       "391            American Airlines & JetBlue: Le Divorce        b   \n",
       "400  JetBlue airplanes at their gates at John F. Ke...        b   \n",
       "402  JetBlue and American Airlines go their differe...        b   \n",
       "415  Is spitting in TriMet driver's face 'assault'?...        b   \n",
       "\n",
       "                                         STOPPED_WORDS PREDICTED  \n",
       "24                                thing,need,know,morn         e  \n",
       "34                           thing,need,know,open,bell         e  \n",
       "48                                                 way         e  \n",
       "49                          ebay,spin,paypal,make,sens         t  \n",
       "133               bull,market,celebr,th,birthday,parti         e  \n",
       "167                                   predict,new,week         e  \n",
       "178                      happi,th,birthday,bull,market         e  \n",
       "187                       bull,market,mark,th,birthday         e  \n",
       "192                                happi,birthday,bull         e  \n",
       "225  hacker,leak,mt,gox,databas,reveal,blog,former,ceo         t  \n",
       "254                  hacker,hit,web,account,mtgox,boss         t  \n",
       "263  mt,gox,ceo,blog,hack,databas,leak,claim,k,bitcoin         t  \n",
       "280                          massiv,hack,attack,reveal         t  \n",
       "283  anonym,hacker,uncov,alleg,proof,mtgox,fraud,si...         t  \n",
       "335                       metro,north,work,kill,harlem         e  \n",
       "369                                      without,trace         e  \n",
       "391                   american,airlin,jetblu,le,divorc         e  \n",
       "400  jetblu,airplan,gate,john,f,kennedi,airport,new...         e  \n",
       "402               jetblu,american,airlin,go,differ,way         e  \n",
       "415  spit,trimet,driver,face,assault,union,want,hos...         e  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = list(classifier.misclassified.index)[:20]\n",
    "pd.concat([headlines.iloc[indices,[0,2,3]], classifier.misclassified.iloc[:20,2]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if classifier also works on random title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': -56.15393854758061, 'b': -45.6394781007972, 'm': -59.29287028123265, 't': -48.8739788142794}\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predictCats('Apple posts higher returns this quarter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, business has the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': -56.453691188096094, 'b': -72.27064568314503, 'm': -75.1939873572149, 't': -68.20941069184059}\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predictCats('Tom Cruise stars in upcoming action movie'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entertainment wins!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
